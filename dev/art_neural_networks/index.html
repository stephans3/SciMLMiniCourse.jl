<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Artificial Neural Networks · SciMLMiniCourse.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://stephans3.github.io/SciMLMiniCourse.jl/art_neural_networks/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">SciMLMiniCourse.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../intro_julia/">Quick Julia introduction</a></li><li><a class="tocitem" href="../diff_eq/">Differential Equations</a></li><li><a class="tocitem" href="../homework_1/">Homework: Day 1</a></li><li class="is-active"><a class="tocitem" href>Artificial Neural Networks</a><ul class="internal"><li><a class="tocitem" href="#Linear-regression-in-1D"><span>Linear regression in 1D</span></a></li><li><a class="tocitem" href="#Linear-regression-in-2D"><span>Linear regression in 2D</span></a></li><li><a class="tocitem" href="#Nonlinear-function-approximation"><span>Nonlinear function approximation</span></a></li><li><a class="tocitem" href="#Flux-vs.-Lux"><span>Flux vs. Lux</span></a></li></ul></li><li><a class="tocitem" href="../neural_ode/">Neural Differential Equaions</a></li><li><a class="tocitem" href="../homework_2/">Homework: Day 2</a></li><li><a class="tocitem" href="../projects/">Projects</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Artificial Neural Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Artificial Neural Networks</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/stephans3/SciMLMiniCourse.jl/blob/main/docs/src/art_neural_networks.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Artificial-Neural-Networks"><a class="docs-heading-anchor" href="#Artificial-Neural-Networks">Artificial Neural Networks</a><a id="Artificial-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Artificial-Neural-Networks" title="Permalink"></a></h1><p>... are a set of machine learning tools which are <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">&quot;inspired by biological neural networks&quot;, see Wikipedia</a>.  They can be visualized as (directed) graphs where the nodes or vertices are neurons and the edges are connections between the neurons.</p><p>In this course we will focus on (simple) feed-forward networks with</p><ul><li>one input layer consisting of <span>$N$</span> input neurons</li><li>a certain number of hidden layers, and</li><li>one ouput layer consisting of <span>$M$</span> output neurons.</li></ul><p>The <span>$j$</span>-th neuron of a hidden layer or output layer <span>$l$</span> has three components:</p><ul><li>a connection from the <span>$k$</span>-th neuron of the previous layer <span>$l-1$</span> with weight <span>$w_{j,k}^{l}$</span>,</li><li>a bias <span>$b_{j}^{l}$</span> and</li><li>an <a href="https://en.wikipedia.org/wiki/Activation_function">activation function <span>$f^{l}(\cdot)$</span></a> like <span>$tanh$</span> or sigmoid function.</li></ul><p>The state of this neuron is computed as </p><p class="math-container">\[x_{j}^{l} = f^{l}(w_{j,k}^{l} ~ x_{k}^{l-1} + b_{j}^{l}).\]</p><p>The weights and the bias are usually expressed in matrix and vector notation as </p><p class="math-container">\[W^{l} = \begin{pmatrix}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,k} &amp; \cdots &amp; w_{1,K} \\
\vdots  &amp; \vdots  &amp; ~      &amp; ~       &amp;        &amp; \vdots  \\
w_{j,1} &amp; w_{j,2} &amp; \cdots &amp; w_{j,k} &amp; \cdots &amp; w_{j,K} \\
\vdots  &amp; \vdots  &amp; ~      &amp;         &amp;        &amp; \vdots  \\
w_{J,1} &amp; w_{J,2} &amp; \cdots &amp; w_{J,k} &amp; \cdots &amp; w_{J,K}
\end{pmatrix} \qquad b^{l} = \begin{pmatrix}
b_{1,1} \\
\vdots  \\
b_{j,1} \\
\vdots  \\
b_{J,1} 
\end{pmatrix}\]</p><p>Weights and bias are parameters which are adjusted during the training of the neural network to minimize the difference between the recent output data <span>$y$</span> and the target output <span>$y_{target}$</span>. </p><p><strong>Procedure</strong></p><ol><li>Multiply the input data <span>$x^{0}$</span> with the weights between the input layer and first hidden layer as <span>$W^{1} x^{0} + b^{1}$</span></li><li>Calculate output of the first hidden layer <span>$x^{1} = f^{1}(W^{1} x^{0} +  + b^{1})$</span></li><li>Repeat step 1 and 2 for all hidden layers and the output layers</li></ol><p>Therefore the output data after the last layer <span>$L$</span> is found as </p><p class="math-container">\[y = f^{L}( W^{L} f^{L-1}( W^{L-1} f^{L-2}( \cdots f^{1}(W^{1} x^{0} + b^{1} ) + b^{L-2} ) + b^{L-1} ) + b^{L} ).\]</p><p>The computed output data <span>$y$</span> is compared with the target output <span>$y_{target}$</span>, and the difference is evaluated with a loss function or cost function. For example we use the <a href="https://en.wikipedia.org/wiki/Loss_function#Quadratic_loss_function">quadratic loss function</a></p><p class="math-container">\[J(y_{target,i},y) = \sum_{i=1} (y_{target,i} - y_{i})^2\]</p><p>The weights are adjusted and the loss function is minimized using backpropagation and optimization methods like (stochastic) <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>. </p><h2 id="Linear-regression-in-1D"><a class="docs-heading-anchor" href="#Linear-regression-in-1D">Linear regression in 1D</a><a id="Linear-regression-in-1D-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression-in-1D" title="Permalink"></a></h2><p>The first example is taken from the <a href="https://fluxml.ai/Flux.jl/stable/models/overview/">Flux documentation</a>. Here, we try to approximate the function</p><p class="math-container">\[f(x) = 4~x + 2\]</p><p>with an artificial neural network with 1 input and 1 output neuron. The artificial neural network model can be noted as</p><p class="math-container">\[y = W^{1} x^{0} + b^{1}\]</p><p>where <span>$W^{1}$</span> and <span>$b^{1}$</span> are the parameters that shall be learned and <span>$x^{0}$</span> is the input data.</p><h2 id="Linear-regression-in-2D"><a class="docs-heading-anchor" href="#Linear-regression-in-2D">Linear regression in 2D</a><a id="Linear-regression-in-2D-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression-in-2D" title="Permalink"></a></h2><p>In the second example we try to approximate the two-dimensinal function</p><p class="math-container">\[f(x,y) = 3~x - 5~y + 2\]</p><p>with the artificial neural network</p><p class="math-container">\[y = (W_{1,1}^{1}, W_{1,2}^{1}) \begin{pmatrix}
x_{1}^{0}\\
x_{2}^{0}
\end{pmatrix} + b^{1}.\]</p><p>Here we have 2 input neurons and 1 output neuron. Next, we see how to build an ANN model step-by-step.</p><h4 id="Generate-data"><a class="docs-heading-anchor" href="#Generate-data">Generate data</a><a id="Generate-data-1"></a><a class="docs-heading-anchor-permalink" href="#Generate-data" title="Permalink"></a></h4><p>Firstly, we define the true model and create random input values for training and test. Here, we define the interval for training data as <span>$[-3,4]$</span> and for test data as <span>$[2,9]$</span>. The lower and upper bounds of the intervals are chosen arbitrarily. </p><pre><code class="language-julia hljs">actual(x) = 3x[1] - 5x[2] + 2  # Model that shall be approximated
N_samples = 10
x_train, x_test = rand(-3:0.1:4, 2, N_samples), rand(2:0.1:9, 2, N_samples) # Input data</code></pre><p>Next, we need to generate the output data which is used later to compute the loss. </p><pre><code class="language-julia hljs">y_train, y_test = zeros(1,N_samples), zeros(1,N_samples) # Output data
for i in 1:N_samples
    y_train[i] = actual(x_train[:,i])
    y_test[i] = actual(x_test[:,i])
end</code></pre><h4 id="ANN-model-and-loss-function"><a class="docs-heading-anchor" href="#ANN-model-and-loss-function">ANN model and loss function</a><a id="ANN-model-and-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#ANN-model-and-loss-function" title="Permalink"></a></h4><p>In the second step, we create the artificial neural network model. As discussed above, we have 2 input neurons and 1 output neuron. We have in total 3 parameters: <span>$W_{1,1}^{1}, W_{1,2}^{1}$</span> and <span>$b_{1}^{1}$</span>. We do not need an activation function because our reference model is linear.</p><pre><code class="language-julia hljs">using Flux
predict = Dense(2 =&gt; 1) # ANN with 2 input neuron and 1 output neuron
parameters = Flux.params(predict) # Parameters of ANN</code></pre><p>Further, we use a <a href="https://en.wikipedia.org/wiki/Mean_squared_error">mean squared error</a> as</p><p class="math-container">\[J(y_{t},y) = \sum (y_{t,i} - y_{i})^2.\]</p><pre><code class="language-julia hljs">loss(x, y) = Flux.Losses.mse(predict(x), y); # Loss function: mean squared error</code></pre><h4 id="Train-model"><a class="docs-heading-anchor" href="#Train-model">Train model</a><a id="Train-model-1"></a><a class="docs-heading-anchor-permalink" href="#Train-model" title="Permalink"></a></h4><p>In the last step, we choose <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> as the optimization method, define the input and output training data and run 1 training iteration.</p><pre><code class="language-julia hljs">using Flux: train!
opt = Descent() # Gradient descent optimizer
data = [(x_train, y_train)]
train!(loss, parameters, data, opt)</code></pre><p>The training step has to be repeated iteratively to minimize the loss.</p><h2 id="Nonlinear-function-approximation"><a class="docs-heading-anchor" href="#Nonlinear-function-approximation">Nonlinear function approximation</a><a id="Nonlinear-function-approximation-1"></a><a class="docs-heading-anchor-permalink" href="#Nonlinear-function-approximation" title="Permalink"></a></h2><p>The approximation of linear functions is very simple because we only do not need a hidden layer - input and output layer are enough. In case of nonlinear functions we need hidden layers and activation functions. In this example we assume the nonlinear function</p><p class="math-container">\[f(x) = 3 - 0.5~x - 1.5 ~ x^2 + 0.5 ~ x^3.\]</p><p>In this example, we use  an artificial neural network with one hidden layer. The ANN structure is:</p><ul><li>Input layer with 1 neuron</li><li>2 hidden layers with <span>$N$</span> neurons</li><li>Output layer with 1 neuron</li></ul><p>The ANN model is modelled with <code>Chain()</code> with <span>$N$</span> in the hidden layer:</p><pre><code class="language-julia hljs">Nn = 64 # 16 # Number of neurons per layer
model = Chain(
  Dense(1 =&gt; Nn, σ),
  Dense(Nn =&gt; Nn, σ),
  Dense(Nn =&gt; 1))</code></pre><p>In the output layer <span>$L$</span> we sum up all results of the previous layer <span>$L-1$</span> like</p><p class="math-container">\[y = \sum_{n} [W_{1,n}^{L} ~ f_{n}^{L-1}(W^{L-1} x^{L-2} + b^{L-1})] + b_{1,1}^{L}. \]</p><p>Learning the weights and bias is typical function approximation or interpolation. And the more neurons we have in the hidden layer the more exact is our interpolation. However, this approach only works fine here for interpolation and not for extrapolation. </p><h2 id="Flux-vs.-Lux"><a class="docs-heading-anchor" href="#Flux-vs.-Lux">Flux vs. Lux</a><a id="Flux-vs.-Lux-1"></a><a class="docs-heading-anchor-permalink" href="#Flux-vs.-Lux" title="Permalink"></a></h2><p>Flux is one Julia&#39;s standard machine learning libraries. Beside Flux there exist several other libraries like:</p><ul><li><a href="https://alan-turing-institute.github.io/MLJ.jl/dev/">MLJ</a></li><li><a href="https://denizyuret.github.io/Knet.jl/latest/">Knet</a></li><li><a href="http://lux.csail.mit.edu/stable/">Lux</a></li></ul><p>In this course we will use <strong>Flux</strong> and <strong>Lux</strong>. Lux is a young project that was created to build a better foundation for Scientific Machine Learning methods than Flux. This means, the interoperability with <strong>DifferentialEquations</strong> framework shall be better than with Flux.</p><h4 id="Further-reading"><a class="docs-heading-anchor" href="#Further-reading">Further reading</a><a id="Further-reading-1"></a><a class="docs-heading-anchor-permalink" href="#Further-reading" title="Permalink"></a></h4><p>The Knet documentation contains a rather comprehensive introduction to artificial neural networks and their training methods. It can be found here: <a href="https://denizyuret.github.io/Knet.jl/latest/backprop/">Backpropagation and SGD</a></p><p>If you want to play more with Flux, you may take a look on the <a href="https://github.com/FluxML/model-zoo">Flux model zoo</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../homework_1/">« Homework: Day 1</a><a class="docs-footer-nextpage" href="../neural_ode/">Neural Differential Equaions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 4 October 2022 08:34">Tuesday 4 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
