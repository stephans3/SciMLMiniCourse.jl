var documenterSearchIndex = {"docs":
[{"location":"diff_eq/#Differential-Equations","page":"Differential Equations","title":"Differential Equations","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Differential Equations are mathematical structures which can be used to model physical, biological, chemical, economical/financial and social models. ","category":"page"},{"location":"diff_eq/#[Ordinary-Differential-Equations-(ODE)](https://en.wikipedia.org/wiki/Ordinary_differential_equation)","page":"Differential Equations","title":"Ordinary Differential Equations (ODE)","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Van der Pol oscillator","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"ddoty(t) - mu (1 - y(t)^2) doty(t) + y(t) = 0","category":"page"},{"location":"diff_eq/#[Differential-Algebraic-Equation-(DAE)](https://en.wikipedia.org/wiki/Differential-algebraic_system_of_equations)","page":"Differential Equations","title":"Differential-Algebraic Equation (DAE)","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"beginmatrix\ndotx(t) =  -2x(t) + 3y(t) \ndoty(t) =  -3x(t) \n0 =  x(t) + y(t)\nendmatrix","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Applications e.g. in Robotics (multibody dynamics) and Electrical Engineering (circuit modeling).","category":"page"},{"location":"diff_eq/#[Stochastic-Differential-Equation-(SDE)](https://en.wikipedia.org/wiki/Stochastic_differential_equation)","page":"Differential Equations","title":"Stochastic Differential Equation (SDE)","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Langevin equation","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"m fracd v(t)dt = - lambda v(t) + eta(t)","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"with noise term eta(t)","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Applications e.g. in biology. ","category":"page"},{"location":"diff_eq/#[Partial-Differential-Equation-(PDE)](https://en.wikipedia.org/wiki/Partial_differential_equation)","page":"Differential Equations","title":"Partial Differential Equation (PDE)","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Heat Equation ","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"fracpartial y(tx)partial t = alpha fracpartial^2 y(tx)partial x^2","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Applications in continuum mechanics (Navier-Stokes equations), electromagnetism (Maxwell–Heaviside equations), etc. ","category":"page"},{"location":"diff_eq/#Mixtures-and-other-types","page":"Differential Equations","title":"Mixtures and other types","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Stochastic PDE\nPartial-Differential-Algebraic Equations \nIntegro-differential equations\nDelay differential equations","category":"page"},{"location":"diff_eq/#About-ODEs","page":"Differential Equations","title":"About ODEs","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"In this course we will focus on ODEs. Nevertheless, the existing SciML methods can also be used for other types of differential equations. As a first step in the study of an ODE we distinguish between linear and nonlinear equations. Many easy concepts from linear algebra exist to investigate linear ODEs. These concepts can not (or only up to some limit) be applied on nonlinear ODE and so we have to use methods from systems theory (Lyapunov stability) or differential geometry like Lie derivatives, differential flatness, etc.","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"If we study a linear ODE dotx(t) = A  x(t) we can determine the behaviour of its x(t) for t rightarrow infty via its stability (in the sense of Lyapunov). If (and only iff) all Eigenvalues of system matrix A smaller than zero (negative) then we call call the ODE stable and we know that all states approaching zero at t rightarrow infty","category":"page"},{"location":"diff_eq/#Example:-Linear-ODE","page":"Differential Equations","title":"Example: Linear ODE","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The ordinary differential equation","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"beginpmatrix\ndotx_1(t) \ndotx_2(t)\nendpmatrix =\nbeginpmatrix\n-2  1 \n3  -4 \nendpmatrix\nbeginpmatrix\nx_1(t) \nx_2(t)\nendpmatrix","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"has the Eigenvalues lambda in -1 -5. So the ODE is stable and all states reach for arbitrary initial values x_0 the origin (00).","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"In Julia we can prove this with method eigvals() from the standard library LinearAlgebra.jl.","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"A = [-2 1; 3 -4];\n\nusing LinearAlgebra\nevals = eigvals(A)","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The true solution of this ODE is x(t)=exp(At)  x_0 ","category":"page"},{"location":"diff_eq/#Euler-method","page":"Differential Equations","title":"Euler method","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The differential equation dotx(t)=Ax(t) can also be solved numerically. The forward Euler method is one of the easiest numerical integration methods. It is briefly described by","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"dotx(t) approx fracx(n+1) - x(n)Delta T = A  x(n)","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"and can be rearranged as","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"x(n+1) = x(n) + Delta T  Ax(n) = (I + Delta T  A)  x(n)","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"with identity matrix I. The sampling time Delta T0 has to be chosen such that all Eigenvalues of matrix A_d = I + Delta T  A are inside the unit circle. So, we guarantee a stable numerical integration; see also: A-Stability.","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Tf = 10.0;  # Final simulation time\nΔT = 0.25;  # Sampling time\nN  = round(Int, Tf/ΔT + 1); # Number of sampling points\nx = zeros(2, N)       # Solution of Euler method\nx[:,1] = x0           # Set initial value\n\nfor i in range(1,N-1)\n    x[:,i+1] = x[:,i] + ΔT * A*x[:,i] # Euler method: x(n+1)=x(n)+ΔT*A*x(n)\nend","category":"page"},{"location":"diff_eq/#Runge-Kutta-methods-via-DifferentialEquations.jl","page":"Differential Equations","title":"Runge-Kutta methods via DifferentialEquations.jl","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The Euler method has a very weak performance and is rather used for tests. In contrast, n-th order Runge-Kutta methods provide much more better integration tools. A lot of integration methods is already implemented in DifferentialEquations.jl. A list of all integration methods can be found in section \"ODE Solvers\". ","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"I recommend to read the tutorial about Ordinary Differential Equations.","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"using DifferentialEquations\nlinear_ode(x,p,t) = A * x   # Right-hand side of ODE x'(t)=A x(t)\ntspan = (0.,Tf)             # Time span \nalg   = Tsit5()             # Runge-Kutta integration method\nprob  = ODEProblem(linear_ode, x0, tspan)\nsol   = solve(prob, alg, saveat=ΔT)    # Solution of numerical integration","category":"page"},{"location":"diff_eq/#Parameters","page":"Differential Equations","title":"Parameters","text":"","category":"section"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"One main questions of this course is how to compute system parameters. We know for many processes the system dynamics which is described by parameters. However, quite often we do not know the certain parameters. In case of a spring mass system we derive the ODE as","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"m ddotx(t) + k x(t) = u(t)","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"or equivalent ","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"ddotx(t) = -frackm x(t) + u(t) = -p  x(t) + u(t)","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"with mass m0 and spring constant k0 as parameters and u(t) as external force (or input signal). Both parameters are united as p=frackm. We assume the input u(t)=sin(t) and develop the right-hand side of this ODE.","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"function mass_spring(dx, x, p, t)\n    u = sin(t)   # external force / input signal\n    \n    dx[1] = x[2]                # dx1/dt = x2\n    dx[2] = -p[1] * x[1] + u    # dx2/dt = -p * x2 + u\nend","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The time span, initial values x_0 = (00)^top and the parameter p=frackm=2 have to be defined.","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"param = [2.0]       # Parameter p = k/m with k=spring constant, m=mass\nx0    = [0.0, 0.0]  # Initial values\ntspan = (0.,30.)    # Time span ","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The parameter can be handed over to the solver in ODEProblem(...,p=param) or in solve(,p=param). I recommend to use solve(,p=param) because in the Machine Learning part we will build the ODEProblem once and use solve repeatedly.","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"using DifferentialEquations\nalg   = Tsit5()             # Runge-Kutta integration method\nprob  = ODEProblem(mass_spring, x0, tspan)\nsol   = solve(prob, alg, p=param)    # Numerical integration with parameter","category":"page"},{"location":"diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"See also: (Defining Parametrized Functions)[https://diffeq.sciml.ai/stable/tutorials/ode_example/#Defining-Parameterized-Functions] in the DifferentialEquations docs.","category":"page"},{"location":"homework_1/#Homework:-Day-1","page":"Homework: Day 1","title":"Homework: Day 1","text":"","category":"section"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"Read the DifferentialEquations.jl documentation about Ordinary Differential Equations and solve the following exercises.","category":"page"},{"location":"homework_1/#Linear-ODE","page":"Homework: Day 1","title":"Linear ODE","text":"","category":"section"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"We assume the ordinary differential equation","category":"page"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"dotx_1(t) =  -x_1 + 2  x_2 + 3  x_3 \ndotx_2(t) = 4x_1 - 5x_2 + 6x_3 \ndotx_3(t) = 7x_1 + 8x_2 - 9x_3","category":"page"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"Tasks","category":"page"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"Is this ODE stable? Compute the Eigenvalues.\nFind the analytical solution for initial values x(0)=(111)^top.\nDiscretize the system with Euler method and simulate it for T_f=5 seconds.\nPlot the analytical solution and the solution of the Euler method. You may use the option yaxis=:log10 in plot() for a better readability.\nCompute the solution with DifferentialEquations.jl or OrdinaryDiffEq.jl (which is a sub-library of DifferentialEquations.jl).","category":"page"},{"location":"homework_1/#Mathematical-Pendulum","page":"Homework: Day 1","title":"Mathematical Pendulum","text":"","category":"section"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"We assume the mathematical pendulum","category":"page"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"dotphi(t) = omega(t) \ndotomega(t) = -fracgl  sin(phi(t))","category":"page"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"with angle phi, angle velocity omega, gravitational acceleration g=981 and lenght l0. ","category":"page"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"Tasks","category":"page"},{"location":"homework_1/","page":"Homework: Day 1","title":"Homework: Day 1","text":"Compute the solution with DifferentialEquations.jl or OrdinaryDiffEq.jl\nAssume the initial values (phi_0omega_0)=(π40)^top and simulation time T_f=5 seconds.\nUse parameter p=fracgl with l=01.\nPlot your results.\nVary the length from l=01 to l=1 meter.","category":"page"},{"location":"projects/#Course-Projects","page":"Projects","title":"Course Projects","text":"","category":"section"},{"location":"projects/#Augmented-Neural-Ordinary-Differential-Equations","page":"Projects","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint","category":"page"},{"location":"projects/#Continuous-Normalizing-Flows-/-FFJORD","page":"Projects","title":"Continuous Normalizing Flows / FFJORD","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia:Flow-based generative model","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint: Neural ODE -> About Continuous Normalizing Flows","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint: FFJORD","category":"page"},{"location":"projects/#Hamiltonian-Neural-Network","page":"Projects","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint","category":"page"},{"location":"projects/#Physics-Informed-Machine-Learning-(PIML)-with-TensorLayer","page":"Projects","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/#Neural-Graph-Differential-Equations","page":"Projects","title":"Neural Graph Differential Equations","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"GraphNeuralNetworks.jl","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Graph neural network","category":"page"},{"location":"projects/#Partial-Differential-Equation-(PDE)-Constrained-Optimization","page":"Projects","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Heat equation","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Finite Differences method","category":"page"},{"location":"projects/#Solving-Optimal-Control-Problems-with-Universal-Differential-Equations","page":"Projects","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Optimal Control","category":"page"},{"location":"art_neural_networks/#Artificial-Neural-Networks","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"... are a set of machine learning tools which are \"inspired by biological neural networks\", see Wikipedia.  They can be visualized as (directed) graphs where the nodes or vertices are neurons and the edges are connections between the neurons.","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In this course we will focus on (simple) feed-forward networks with","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"one input layer consisting of N input neurons\na certain number of hidden layers, and\none ouput layer consisting of M output neurons.","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The j-th neuron of a hidden layer or output layer l has three components:","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"a connection from the k-th neuron of the previous layer l-1 with weight w_jk^l,\na bias b_j^l and\nan activation function f^l(cdot) like tanh or sigmoid function.","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The state of this neuron is computed as ","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"x_j^l = f^l(w_jk^l  x_k^l-1 + b_j^l)","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The weights and the bias are usually expressed in matrix and vector notation as ","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"W^l = beginpmatrix\nw_11  w_12  cdots  w_1k  cdots  w_1K \nvdots   vdots                          vdots  \nw_j1  w_j2  cdots  w_jk  cdots  w_jK \nvdots   vdots                           vdots  \nw_J1  w_J2  cdots  w_Jk  cdots  w_JK\nendpmatrix qquad b^l = beginpmatrix\nb_11 \nvdots  \nb_j1 \nvdots  \nb_J1 \nendpmatrix","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Weights and bias are parameters which are adjusted during the training of the neural network to minimize the difference between the recent output data y and the target output y_target. ","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Procedure","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Multiply the input data x^0 with the weights between the input layer and first hidden layer as W^1 x^0 + b^1\nCalculate output of the first hidden layer x^1 = f^1(W^1 x^0 +  + b^1)\nRepeat step 1 and 2 for all hidden layers and the output layers","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Therefore the output data after the last layer L is found as ","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = f^L( W^L f^L-1( W^L-1 f^L-2( cdots f^1(W^1 x^0 + b^1 ) + b^L-2 ) + b^L-1 ) + b^L )","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The computed output data y is compared with the target output y_target, and the difference is evaluated with a loss function or cost function. For example we use the quadratic loss function","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"J(y_targetiy) = sum_i=1 (y_targeti - y_i)^2","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The weights are adjusted and the loss function is minimized using backpropagation and optimization methods like (stochastic) gradient descent. ","category":"page"},{"location":"art_neural_networks/#Linear-regression-in-1D","page":"Artificial Neural Networks","title":"Linear regression in 1D","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The first example is taken from the Flux documentation. Here, we try to approximate the function","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"f(x) = 4x + 2","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"with an artificial neural network with 1 input and 1 output neuron. The artificial neural network model can be noted as","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = W^1 x^0 + b^1","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"where W^1 and b^1 are the parameters that shall be learned and x^0 is the input data.","category":"page"},{"location":"art_neural_networks/#Linear-regression-in-2D","page":"Artificial Neural Networks","title":"Linear regression in 2D","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the second example we try to approximate the two-dimensinal function","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"f(xy) = 3x - 5y + 2","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"with the artificial neural network","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = (W_11^1 W_12^1) beginpmatrix\nx_1^0\nx_2^0\nendpmatrix + b^1","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Here we have 2 input neurons and 1 output neuron. Next, we see how to build an ANN model step-by-step.","category":"page"},{"location":"art_neural_networks/#Generate-data","page":"Artificial Neural Networks","title":"Generate data","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Firstly, we define the true model and create random input values for training and test. Here, we define the interval for training data as -34 and for test data as 29. The lower and upper bounds of the intervals are chosen arbitrarily. ","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"actual(x) = 3x[1] - 5x[2] + 2  # Model that shall be approximated\nN_samples = 10\nx_train, x_test = rand(-3:0.1:4, 2, N_samples), rand(2:0.1:9, 2, N_samples) # Input data","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Next, we need to generate the output data which is used later to compute the loss. ","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y_train, y_test = zeros(1,N_samples), zeros(1,N_samples) # Output data\nfor i in 1:N_samples\n    y_train[i] = actual(x_train[:,i])\n    y_test[i] = actual(x_test[:,i])\nend","category":"page"},{"location":"art_neural_networks/#ANN-model-and-loss-function","page":"Artificial Neural Networks","title":"ANN model and loss function","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the second step, we create the artificial neural network model. As discussed above, we have 2 input neurons and 1 output neuron. We have in total 3 parameters: W_11^1 W_12^1 and b_1^1. We do not need an activation function because our reference model is linear.","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"using Flux\npredict = Dense(2 => 1) # ANN with 2 input neuron and 1 output neuron\nparameters = Flux.params(predict) # Parameters of ANN","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Further, we use a mean squared error as","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"J(y_ty) = sum (y_ti - y_i)^2","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"loss(x, y) = Flux.Losses.mse(predict(x), y); # Loss function: mean squared error","category":"page"},{"location":"art_neural_networks/#Train-model","page":"Artificial Neural Networks","title":"Train model","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the last step, we choose gradient descent as the optimization method, define the input and output training data and run 1 training iteration.","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"using Flux: train!\nopt = Descent() # Gradient descent optimizer\ndata = [(x_train, y_train)]\ntrain!(loss, parameters, data, opt)","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The training step has to be repeated iteratively to minimize the loss.","category":"page"},{"location":"art_neural_networks/#Nonlinear-function-approximation","page":"Artificial Neural Networks","title":"Nonlinear function approximation","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The approximation of linear functions is very simple because we only do not need a hidden layer - input and output layer are enough. In case of nonlinear functions we need hidden layers and activation functions. In this example we assume the nonlinear function","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"f(x) = 3 - 05x - 15  x^2 + 05  x^3","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In this example, we use  an artificial neural network with one hidden layer. The ANN structure is:","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Input layer with 1 neuron\n2 hidden layers with N neurons\nOutput layer with 1 neuron","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The ANN model is modelled with Chain() with N in the hidden layer:","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Nn = 64 # 16 # Number of neurons per layer\nmodel = Chain(\n  Dense(1 => Nn, σ),\n  Dense(Nn => Nn, σ),\n  Dense(Nn => 1))","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the output layer L we sum up all results of the previous layer L-1 like","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = sum_n W_1n^L  f_n^L-1(W^L-1 x^L-2 + b^L-1) + b_11^L ","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Learning the weights and bias is typical function approximation or interpolation. And the more neurons we have in the hidden layer the more exact is our interpolation. However, this approach only works fine here for interpolation and not for extrapolation. ","category":"page"},{"location":"art_neural_networks/#Flux-vs.-Lux","page":"Artificial Neural Networks","title":"Flux vs. Lux","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Flux is one Julia's standard machine learning libraries. Beside Flux there exist several other libraries like:","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"MLJ\nKnet\nLux","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In this course we will use Flux and Lux. Lux is a young project that was created to build a better foundation for Scientific Machine Learning methods than Flux. This means, the interoperability with DifferentialEquations framework shall be better than with Flux.","category":"page"},{"location":"art_neural_networks/#Further-reading","page":"Artificial Neural Networks","title":"Further reading","text":"","category":"section"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The Knet documentation contains a rather comprehensive introduction to artificial neural networks and their training methods. It can be found here: Backpropagation and SGD","category":"page"},{"location":"art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"If you want to play more with Flux, you may take a look on the Flux model zoo.","category":"page"},{"location":"neural_ode/#Neural-Ordinary-Differential-Equations","page":"Neural Differential Equaions","title":"Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"Neural Ordinary Differential Equations (Neural ODE) are either:","category":"page"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"artificial neural networks that are modelled as ordinary differential equations (used in machine learning) or\nordinary differential equations that contain neural networks (used in computational sciences and engineering).","category":"page"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"The idea of neural ordinary differential equations was firstly introduced (as we use it today) by Chen, Rubanova, Bettencourt and Duvenaud in 2018. The arxiv preprint can be found here: Neural Ordinary Differential Equations.","category":"page"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"The basic idea was to speed-up the computation of deep learning approaches like residual neural networks, recurrent neural networks or normalizing flows. All of these appraches can be described by ","category":"page"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"x(n+1) = x(n) + f(x(n) p(n))","category":"page"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"where n denotes the layer, x the states, p the parameters and f() the activation function. This recursive algorithm x(n+1)= also describes the forward Euler method which is used to solve ordinary differential equations. So, the idea is to formulate the original ordinary differential equation","category":"page"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"fracdx(t)dt = f(x(t) pt)","category":"page"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"and solve it with a high-order numerical integration method like Runge-Kutta methods. ","category":"page"},{"location":"neural_ode/#Literature","page":"Neural Differential Equaions","title":"Literature","text":"","category":"section"},{"location":"neural_ode/","page":"Neural Differential Equaions","title":"Neural Differential Equaions","text":"Neural Ordinary Differential Equations\nUniversal Differential Equations\nOn Neural Differential Equations","category":"page"},{"location":"homework_2/#Homework:-Day-2","page":"Homework: Day 2","title":"Homework: Day 2","text":"","category":"section"},{"location":"homework_2/#dimensional-function-approximation","page":"Homework: Day 2","title":"2-dimensional function approximation","text":"","category":"section"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Read in the Flux documentation about building models:","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Overview\nBasics ","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"We assume the true model","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"    f(xy) = 200xfraccos(2x)exp(-x^2)1 + exp(-2x)sin(yexp(-y^2)) ","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Tasks","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Create random training values for (xy) in -22 times -22\nBuild an artificial neural network model (with Flux) with at least 1 hidden layer\nTrain the ANN model and plot the loss values per iteration. Use at least 5000 iterations.\nPlot the graph of the ANN model and the true model.","category":"page"},{"location":"homework_2/#Function-approximation-with-Lux.jl","page":"Homework: Day 2","title":"Function approximation with Lux.jl","text":"","category":"section"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Read in the Lux documentation:","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Julia & Lux for the Uninitiated\nFitting a Polynomial using MLP","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Task","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"Implement with Lux.jl the 1-dimensional function approximation example with","category":"page"},{"location":"homework_2/","page":"Homework: Day 2","title":"Homework: Day 2","text":"y = sum_n W_1n^L  f_n^L-1(W^L-1 x^L-2 + b^L-1) + b_11^L","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = SciMLMiniCourse","category":"page"},{"location":"#Mini-Course-on-Scientific-Machine-Learning-(SciML)","page":"Home","title":"Mini Course on Scientific Machine Learning (SciML)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This mini course is based on Julia programming language and covers the following topics","category":"page"},{"location":"","page":"Home","title":"Home","text":"Quick introduction / repetition of differential equations\nSee: DifferentialEquations.jl\nArtificial Neural Networks \nFlux.jl for standard Machine Learning \nLux.jl as ML backend for SciML\nNeural Ordinary Differential Equations \nDiffEqFlux.jl for Neural ODEs\nSciMLSensitivity.jl for Parameter Estimation and Sensitivity Analysis\nOptimization.jl Meta-Package for Optimization tools\nSensitivity Analysis\nForwardDiff.jl for Forward Differentiation\nZygote.jl for Automatic Differentiation","category":"page"},{"location":"#What-is-SciML?","page":"Home","title":"What is SciML?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In Scientific Machine Learning we use ideas and tools from machine learning like artificial neural network, optimizers or automatic differentiation and apply them on problems from natural sciences and engineering. In other words as noted on webpage scientific-ml.com: \"...the development and mathematical theory of machine learning techniques for applications in computational science and engineering\".","category":"page"},{"location":"","page":"Home","title":"Home","text":"In natural sciences and engineering we describe our ideas and models with formulas, for instance differential equations. We run experiments to gain data which we compare with our theoretical model. The more data we have the more precisly we can specify our model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Falling body example","category":"page"},{"location":"","page":"Home","title":"Home","text":"We assume we let an object fall down from x=10 meter, we measure the time until it hits the ground and we want to find the local gravity constant g. This model is described by formula  x(t) = fracg2  t^2 or equally g = frac2t^2  x.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Our time measurements and found gravity constants are","category":"page"},{"location":"","page":"Home","title":"Home","text":"Experiment Time [s] Gravity constant [fracms^2]\n1 1.51 8.77\n2 1.40 10.20\n3 1.39 10.35\n4 1.47 9.26\n5 1.44 9.65","category":"page"},{"location":"","page":"Home","title":"Home","text":"The mean gravity constant is overlineg = fracsum g_i5 approx 965  fracms^2. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"In case of such small and easy models the proposed manual computation may work well. However, in case of complex models like bio-chemical reactions, energy distribution networks or quantum mechanics with hundreds or thousands of formulas and parameters the computation is unknown variables or parameters is much more expensive. So, we need powerful computational tools to handle such problems. ","category":"page"},{"location":"#Learn-about-Julia","page":"Home","title":"Learn about Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are new to Julia then I recommend to learn about it:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Wikibooks: Introducing Julia\nBlog: The Julia Programming Language: an Effective Tutorial\nYoutube: Learn Julia with Us\nMany more resources can be found here","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Computer with Windows/Linux/MacOS/FreeBSD\n(Latest) Julia installation, see: Download Julia\nText editor like Notepad++, Gedit, Visual Studio Code, (Atom)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Not required but can be also used:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Jupyter Notebooks, see IJulia.jl","category":"page"},{"location":"#Further-ressources-about-SciML","page":"Home","title":"Further ressources about SciML","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications\nIntroduction to Computational Thinking\nList of tutorials: SciMLTutorials.jl: Tutorials for Scientific Machine Learning and Differential Equations","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation for SciMLMiniCourse.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [SciMLMiniCourse]","category":"page"},{"location":"intro_julia/#Quick-introduction-to-Julia","page":"Quick Julia introduction","title":"Quick introduction to Julia","text":"","category":"section"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"When you installed the Julia environment on your PC, please open the Julia executive. The REPL will show you something like","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"julia>","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"Here you can try typical commands like","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"julia> 1+1\n2\n\njulia> println(\"Hello World\")\nHello World\n\njulia> foo(x) = x\njulia> foo(3)\n3","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"If you want to develop scripts or packages then it you may use a text editor. You can write some code like","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"a = 3;\nb = 5;\nc = a + b\n\nfunction myfun(x,y)\n    return x+y\nend","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"Save your code, here \"mydemofile.jl\". Now you can open and run your script in the REPL.","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"julia> include(\"path/to_my_file/my_demo_file.jl\")","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"Now, the variables a, b and c and the function myfun are in your environment.","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"julia> a\n3\n\njulia> b\n5\n\njulia> c\n8\n\njulia> myfun(4,5)\n9","category":"page"},{"location":"intro_julia/#Visual-Studio-Code","page":"Quick Julia introduction","title":"Visual Studio Code","text":"","category":"section"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"High level editors like VS Code simplify the usage of Julia. VS Code supports Julia with its extension. See also its documentation for the installation guide. ","category":"page"},{"location":"intro_julia/#Installing-Packages","page":"Quick Julia introduction","title":"Installing Packages","text":"","category":"section"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"Julia has an own Package Manager called Pkg. You can reach it in the REPL if you press ] and you can leave it with backspace. If you want to install a package, e.g. the standard plotting library  Plots then open the REPL and...","category":"page"},{"location":"intro_julia/","page":"Quick Julia introduction","title":"Quick Julia introduction","text":"julia> ]\n\npkg>\n\npkg> add Plots","category":"page"}]
}
