var documenterSearchIndex = {"docs":
[{"location":"intro_programming/num_integration_diff/#Numerical-integration-and-differentiation","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"","category":"section"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"We use vectors and matrices very often to store data or to describe natural or technical processes like diffusion as part of chemical processes or switching of states in Petri nets.   ","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"Many of these processes are based on mathematical tools like differentiation and integration. If we assume the function","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"y(x) = -x^2 + 4","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"with x values in the interval -2 2 then we can directly calculate the integral as","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"int_-2^2 y(x) dx = int_-2^2 -x^2 + 4 dx = left-frac13x^3 + 4x right_-2^2 = frac323 approx 1067","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"Now, we want to calculate this using numerical routines. Firstly, we define the original function","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"y(x)= -x^2 + 4  # Original function","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"and define the vector of data samples with","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"N = 21;                     # Number of samples\nxgrid = range(-2,2,N)       # Grid of x-values\nv = [y(i) for i in xgrid]   # Data samples","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"Next, we approximate the integral as a sum ","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"int_-2^2 y(x) dx approx sum_n=1^N y(x_n)  Delta x","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"where Delta x is the spatial sampling Delta x = x_n+1-xn. We implement this using the sum() function as","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"Δx = xgrid[2]-xgrid[1]  # Spatial sampling\nv_int = sum(v)*Δx       # Approximation of integral","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"We yield for the approximated integral value almost 1064. We can also replace the sum() function with our own function.","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"function mysum(vec)\n    s = 0;  # Store results\n    for (idx,el) in enumerate(vec)\n        s = s + el # Sum up all elements\n    end\n    return s # return result\nend","category":"page"},{"location":"intro_programming/num_integration_diff/#Exercises","page":"Numerical integration and differentiation","title":"Exercises","text":"","category":"section"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"Calculate the integral of normal distribution","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"f(x) = frac1sigma sqrt2piexpleft(-fracx^22sigma^2right) ","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"with sigma=2 in the interval -66.","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"Calculate the integral of Rayleigh distribution","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"f(x) = fracxsigma^2expleft(-fracx^22sigma^2right) ","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"with sigma=2 in the interval -06.","category":"page"},{"location":"intro_programming/num_integration_diff/#Finite-differences","page":"Numerical integration and differentiation","title":"Finite differences","text":"","category":"section"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"The first-order derivative fracddx f(x) can be approximated by the forward finite difference","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"fracddx f(x) approx fracf(x + Delta x) - f(x)Delta x = fracf(x_n+1) - f(x_n)Delta x","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"the backward finite difference","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"fracddx f(x) approx fracf(x) - f(x-Delta x)Delta x = fracf(x_n) - f(x_n-1)Delta x","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"or the central finite difference","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"fracddx f(x) approx \nfracf(x+fracDelta x2) - f(x-fracDelta x2)Delta x  = fracf^+(x_n) - f^-(x_n)Delta x","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"where ","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"f^+(x_n) approx frac12(f(x_n)+ f(x_n+1))","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"and ","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"f^-(x_n) approx frac12(f(x_n-1)+ f(x_n))","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"We are interested in computing the first-order derivate of ","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"y(x) = -x^2 + 4","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"which is found analytically as","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"fracddx y(x) = -2x","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"The finite difference schemes are implemented as functions that use a for-loop to iterate over all data elements. The main difference is the difference scheme. In case of the forward differences we use","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"v_d[i] = (vals[i+1]-vals[i])/dx","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"and in case of the backward differences we note","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"v_d[i-1] = (vals[i]-vals[i-1])/dx","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"For the central differences we firstly have to compute f^+(x_n) and f^-(x_n) and calculate their differences.","category":"page"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"dv1 = (vals[i] + vals[i-1])/2   # First central point: f^{+}\ndv2 = (vals[i] + vals[i+1])/2   # Second central point: f^{-}\nv_d[i-1] = (dv2-dv1)/dx         # Finite difference between first and second central point","category":"page"},{"location":"intro_programming/num_integration_diff/#Forward-finite-differences","page":"Numerical integration and differentiation","title":"Forward finite differences","text":"","category":"section"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"function forward_fd(vals,dx)    # Forward finite differences\n    L = length(vals)    # Length of original vector\n    v_d = zeros(L-1);   # Vector to store finite difference values\n    for i in 1:L-1\n        v_d[i] = (vals[i+1]-vals[i])/dx # Forward difference scheme\n    end\n    return v_d\nend","category":"page"},{"location":"intro_programming/num_integration_diff/#Backward-finite-differences","page":"Numerical integration and differentiation","title":"Backward finite differences","text":"","category":"section"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"function backward_fd(vals,dx)   # Backward finite differences\n    L = length(vals)\n    v_d = zeros(L-1);\n    for i in 2:L\n        v_d[i-1] = (vals[i]-vals[i-1])/dx # Backward difference scheme\n    end\n    return v_d\nend","category":"page"},{"location":"intro_programming/num_integration_diff/#Central-finite-differences","page":"Numerical integration and differentiation","title":"Central finite differences","text":"","category":"section"},{"location":"intro_programming/num_integration_diff/","page":"Numerical integration and differentiation","title":"Numerical integration and differentiation","text":"function central_fd(vals,dx)    # Central finite differences\n    L = length(vals)\n    v_d = zeros(L-2);\n    for i in 2:L-1\n        dv1 = (vals[i] + vals[i-1])/2   # First central point\n        dv2 = (vals[i] + vals[i+1])/2   # Second central point\n        v_d[i-1] = (dv2-dv1)/dx         # Finite difference between first and second central point\n    end\n    return v_d\nend","category":"page"},{"location":"basics_sciml/neural_ode/#Neural-Ordinary-Differential-Equations","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Neural Ordinary Differential Equations (Neural ODE) are either:","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"artificial neural networks that are modelled as ordinary differential equations (used in machine learning) or\nordinary differential equations that contain neural networks (used in computational sciences and engineering).","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"The idea of neural ordinary differential equations was firstly introduced (as we use it today) by Chen, Rubanova, Bettencourt and Duvenaud in 2018. The arxiv preprint can be found here: Neural Ordinary Differential Equations.","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"The basic idea was to speed-up the computation of deep learning approaches like residual neural networks, recurrent neural networks or normalizing flows. All of these appraches can be described by ","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"x(n+1) = x(n) + f(x(n) p(n))","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"where n denotes the layer, x the states, p the parameters and f() the activation function. This recursive algorithm x(n+1)= also describes the forward Euler method which is used to solve ordinary differential equations. So, the idea is to formulate the original ordinary differential equation","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"fracdx(t)dt = f(x(t) pt)","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"and solve it with a high-order numerical integration method like Runge-Kutta methods. ","category":"page"},{"location":"basics_sciml/neural_ode/#Neural-ODE-procedure","page":"Neural Ordinary Differential Equations","title":"Neural ODE procedure","text":"","category":"section"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Build the mathematical problem:\nDefine the neural ODE dotx=f(xt) + g(xpt) with initial states\nDesign the machine learning methods, e.g. neural nets\nChoose an optimization method, e.g. ADAM, BFGS, etc.","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Generate true data from experiments, process data or simulations\nBuild the grey-box model of the system\nknown parts as differential equations\nunknown parts via parameters or ML tools, e.g. neural networks\nCreate the loss function and callback\nDefine used optimization methods and run optimization","category":"page"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Old blog entry with good explanation: DiffEqFlux.jl – A Julia Library for Neural Differential Equations ","category":"page"},{"location":"basics_sciml/neural_ode/#Literature","page":"Neural Ordinary Differential Equations","title":"Literature","text":"","category":"section"},{"location":"basics_sciml/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Neural Ordinary Differential Equations\nUniversal Differential Equations\nOn Neural Differential Equations","category":"page"},{"location":"basics_sciml/diff_eq/#Differential-Equations","page":"Differential Equations","title":"Differential Equations","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Differential Equations are mathematical structures which can be used to model physical, biological, chemical, economical/financial and social models. ","category":"page"},{"location":"basics_sciml/diff_eq/#[Ordinary-Differential-Equations-(ODE)](https://en.wikipedia.org/wiki/Ordinary_differential_equation)","page":"Differential Equations","title":"Ordinary Differential Equations (ODE)","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Van der Pol oscillator","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"ddoty(t) - mu (1 - y(t)^2) doty(t) + y(t) = 0","category":"page"},{"location":"basics_sciml/diff_eq/#[Differential-Algebraic-Equation-(DAE)](https://en.wikipedia.org/wiki/Differential-algebraic_system_of_equations)","page":"Differential Equations","title":"Differential-Algebraic Equation (DAE)","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"beginmatrix\ndotx(t) =  -2x(t) + 3y(t) \ndoty(t) =  -3x(t) \n0 =  x(t) + y(t)\nendmatrix","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Applications e.g. in Robotics (multibody dynamics) and Electrical Engineering (circuit modeling).","category":"page"},{"location":"basics_sciml/diff_eq/#[Stochastic-Differential-Equation-(SDE)](https://en.wikipedia.org/wiki/Stochastic_differential_equation)","page":"Differential Equations","title":"Stochastic Differential Equation (SDE)","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Langevin equation","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"m fracd v(t)dt = - lambda v(t) + eta(t)","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"with noise term eta(t)","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Applications e.g. in biology. ","category":"page"},{"location":"basics_sciml/diff_eq/#[Partial-Differential-Equation-(PDE)](https://en.wikipedia.org/wiki/Partial_differential_equation)","page":"Differential Equations","title":"Partial Differential Equation (PDE)","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Heat Equation ","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"fracpartial y(tx)partial t = alpha fracpartial^2 y(tx)partial x^2","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Applications in continuum mechanics (Navier-Stokes equations), electromagnetism (Maxwell–Heaviside equations), etc. ","category":"page"},{"location":"basics_sciml/diff_eq/#Mixtures-and-other-types","page":"Differential Equations","title":"Mixtures and other types","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Stochastic PDE\nPartial-Differential-Algebraic Equations \nIntegro-differential equations\nDelay differential equations","category":"page"},{"location":"basics_sciml/diff_eq/#About-ODEs","page":"Differential Equations","title":"About ODEs","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"In this course we will focus on ODEs. Nevertheless, the existing SciML methods can also be used for other types of differential equations. As a first step in the study of an ODE we distinguish between linear and nonlinear equations. Many easy concepts from linear algebra exist to investigate linear ODEs. These concepts can not (or only up to some limit) be applied on nonlinear ODE and so we have to use methods from systems theory (Lyapunov stability) or differential geometry like Lie derivatives, differential flatness, etc.","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"If we study a linear ODE dotx(t) = A  x(t) we can determine the behaviour of its x(t) for t rightarrow infty via its stability (in the sense of Lyapunov). If (and only iff) all Eigenvalues of system matrix A smaller than zero (negative) then we call call the ODE stable and we know that all states approaching zero at t rightarrow infty","category":"page"},{"location":"basics_sciml/diff_eq/#Example:-Linear-ODE","page":"Differential Equations","title":"Example: Linear ODE","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The ordinary differential equation","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"beginpmatrix\ndotx_1(t) \ndotx_2(t)\nendpmatrix =\nbeginpmatrix\n-2  1 \n3  -4 \nendpmatrix\nbeginpmatrix\nx_1(t) \nx_2(t)\nendpmatrix","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"has the Eigenvalues lambda in -1 -5. So the ODE is stable and all states reach for arbitrary initial values x_0 the origin (00).","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"In Julia we can prove this with method eigvals() from the standard library LinearAlgebra.jl.","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"A = [-2 1; 3 -4];\n\nusing LinearAlgebra\nevals = eigvals(A)","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The true solution of this ODE is x(t)=exp(At)  x_0 ","category":"page"},{"location":"basics_sciml/diff_eq/#Euler-method","page":"Differential Equations","title":"Euler method","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The differential equation dotx(t)=Ax(t) can also be solved numerically. The forward Euler method is one of the easiest numerical integration methods. It is briefly described by","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"dotx(t) approx fracx(n+1) - x(n)Delta T = A  x(n)","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"and can be rearranged as","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"x(n+1) = x(n) + Delta T  Ax(n) = (I + Delta T  A)  x(n)","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"with identity matrix I. The sampling time Delta T0 has to be chosen such that all Eigenvalues of matrix A_d = I + Delta T  A are inside the unit circle. So, we guarantee a stable numerical integration; see also: A-Stability.","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"Tf = 10.0;  # Final simulation time\nΔT = 0.25;  # Sampling time\nN  = round(Int, Tf/ΔT + 1); # Number of sampling points\nx = zeros(2, N)       # Solution of Euler method\nx[:,1] = x0           # Set initial value\n\nfor i in range(1,N-1)\n    x[:,i+1] = x[:,i] + ΔT * A*x[:,i] # Euler method: x(n+1)=x(n)+ΔT*A*x(n)\nend","category":"page"},{"location":"basics_sciml/diff_eq/#Runge-Kutta-methods-via-DifferentialEquations.jl","page":"Differential Equations","title":"Runge-Kutta methods via DifferentialEquations.jl","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The Euler method has a very weak performance and is rather used for tests. In contrast, n-th order Runge-Kutta methods provide much more better integration tools. A lot of integration methods is already implemented in DifferentialEquations.jl. A list of all integration methods can be found in section \"ODE Solvers\". ","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"I recommend to read the tutorial about Ordinary Differential Equations.","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"using DifferentialEquations\nlinear_ode(x,p,t) = A * x   # Right-hand side of ODE x'(t)=A x(t)\ntspan = (0.,Tf)             # Time span \nalg   = Tsit5()             # Runge-Kutta integration method\nprob  = ODEProblem(linear_ode, x0, tspan)\nsol   = solve(prob, alg, saveat=ΔT)    # Solution of numerical integration","category":"page"},{"location":"basics_sciml/diff_eq/#Parameters","page":"Differential Equations","title":"Parameters","text":"","category":"section"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"One main questions of this course is how to compute system parameters. We know for many processes the system dynamics which is described by parameters. However, quite often we do not know the certain parameters. In case of a spring mass system we derive the ODE as","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"m ddotx(t) + k x(t) = u(t)","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"or equivalent ","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"ddotx(t) = -frackm x(t) + u(t) = -p  x(t) + u(t)","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"with mass m0 and spring constant k0 as parameters and u(t) as external force (or input signal). Both parameters are united as p=frackm. We assume the input u(t)=sin(t) and develop the right-hand side of this ODE.","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"function mass_spring(dx, x, p, t)\n    u = sin(t)   # external force / input signal\n    \n    dx[1] = x[2]                # dx1/dt = x2\n    dx[2] = -p[1] * x[1] + u    # dx2/dt = -p * x2 + u\nend","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The time span, initial values x_0 = (00)^top and the parameter p=frackm=2 have to be defined.","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"param = [2.0]       # Parameter p = k/m with k=spring constant, m=mass\nx0    = [0.0, 0.0]  # Initial values\ntspan = (0.,30.)    # Time span ","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"The parameter can be handed over to the solver in ODEProblem(...,p=param) or in solve(,p=param). I recommend to use solve(,p=param) because in the Machine Learning part we will build the ODEProblem once and use solve repeatedly.","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"using DifferentialEquations\nalg   = Tsit5()             # Runge-Kutta integration method\nprob  = ODEProblem(mass_spring, x0, tspan)\nsol   = solve(prob, alg, p=param)    # Numerical integration with parameter","category":"page"},{"location":"basics_sciml/diff_eq/","page":"Differential Equations","title":"Differential Equations","text":"See also: (Defining Parametrized Functions)[https://diffeq.sciml.ai/stable/tutorials/ode_example/#Defining-Parameterized-Functions] in the DifferentialEquations docs.","category":"page"},{"location":"intro_programming/arrays_loops/#Arrays-and-Loops","page":"Arrays and Loops","title":"Arrays and Loops","text":"","category":"section"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"Julia is made for scientific computation. So, we have Vectors and Matrices which are both subtypes of arrays. We can create a simple 4-element Vector with","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> v = [1,2,3,4]\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"and see that all entries are saved as Int64 which is a 64 bit integer. If we write one value as float then we yield a Vector{Float64} as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> v = [1,2.0,3,4]\n4-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n 4.0","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"In Julia vectors are noted as columns and not as rows. If we do not use the comma between the entries than we yield a matrix with one row as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> v = [1 2 3 4]\n1×4 Matrix{Int64}:\n 1  2  3  4","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"In matrices each row is seperated with semicolons as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> M = [1 2 3 4; 5 6 7 8]\n2×4 Matrix{Int64}:\n 1  2  3  4\n 2  3  4  5","category":"page"},{"location":"intro_programming/arrays_loops/#Zeros,-ones-and-random-values","page":"Arrays and Loops","title":"Zeros, ones and random values","text":"","category":"section"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"If you want to create a vector or a matrix that contains only zero entries then you can use for vectors","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> v = zeros(4)\n4-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"and for matrices","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> M = zeros(4,4)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"If we want to define the datatype then we use zeros(TYPE,ROWS,COLUMNS) as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> M = zeros(Int,4,3)\n4×3 Matrix{Int64}:\n 0  0  0\n 0  0  0\n 0  0  0\n 0  0  0","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"If we want to have a vector or matrix with only one entries then we use ones(TYPE,ROWS,COLUMNS) as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> M = ones(Int,4,3)\n4×3 Matrix{Int64}:\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"We can create random values the same way as before with rand(TYPE,ROWS,COLUMNS) as ","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> M = rand(3,4)\n3×4 Matrix{Float64}:\n 0.545932  0.272659  0.31695   0.707982\n 0.246701  0.92546   0.878453  0.501426\n 0.206801  0.659878  0.174388  0.0598787","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"and with datatype as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"julia> M = rand(Bool, 3,4)\n3×4 Matrix{Bool}:\n 1  0  0  0\n 0  0  0  1\n 1  0  1  1","category":"page"},{"location":"intro_programming/arrays_loops/#For-loops","page":"Arrays and Loops","title":"For-loops","text":"","category":"section"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"We can use while and for loops in Julia. While loops can be used as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"n = 1;\nwhile n<4\n    n = n+1;\n    println(n)\nend","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"However, we will focus in this course only on for loops. There are several ways to define the iteration of the for loop. One way to rewrite the while loop above is ","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"for n=1:3\n    n = n+1;\n    println(n)\nend","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"or using the in statement we can write","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"for n in 1:3\n    n = n+1;\n    println(n)\nend","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"This in statement will be used later at several points. ","category":"page"},{"location":"intro_programming/arrays_loops/#Example:-finding-minimum-and-maximum","page":"Arrays and Loops","title":"Example: finding minimum and maximum","text":"","category":"section"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"Firstly, we create a vector and fill it with some values.","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"# Vector\nv = [1, 3, 5, 7, 9, 0, 2, 4, 6, 8]\nN = length(v) # length of vector","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"The entries can be printed with a for-loop as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"for i=1:N\n    println(\"v[\",i,\"]= \", v[i])\nend","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"or we can use the enumerate function that iterates over the whole vector and returns the index and the element as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"for (idx,el) in enumerate(v)\n    println(\"At index \", idx, \" the element is \", el)\nend","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"The maximum and minimum of the vector entries can be found with maximum() or minimum() as","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"println(\"Maximum of v is \", maximum(v)) \nprintln(\"Minimum of v is \", minimum(v)) ","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"or manually with for-loops. Beside the maximum and minimum value we also want to find the indices of both values. Therefore, we declare 2 variable to store the values and 2 variables to store the indices.","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"    e_max, e_min = -Inf, Inf;   # Init min/max element\n    idx_max, idx_min = -1, -1;  # Init indices","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"for (idx,e) in enumerate(v)\n    # Save recent element and index if it is _larger_ than the already stored one\n    if e > e_max\n        e_max = e\n        idx_max = idx\n    end\n\n    # Save recent element and index if it is _smaller_ than the already stored one\n    if e < e_min\n        e_min = e\n        idx_min = idx\n    end\nend","category":"page"},{"location":"intro_programming/arrays_loops/#Image-processing","page":"Arrays and Loops","title":"Image processing","text":"","category":"section"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"As a simple example we will see you to ","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"using Images\n# This image is taken from: https://openmoji.org/ \n# See: https://openmoji.org/library/emoji-1F34F/\nimg_orig = load(\"images/1F34F_color.png\")\nNpx, Npy = size(img_orig)\n\nnoise_rgb  = rand(RGB,Npx, Npy)  # Noise as random matrix\nnoise_gray = rand(Gray,Npx, Npy) # Noise as random matrix\nimg_noisy  = img_orig+noise_rgb  # Image with RGB noise\n\n# Matrices with gray scale values\nP_orig = channelview(Gray.(img_orig)) + zeros(Npx, Npy)\nP_noisy = channelview(Gray.(img_noisy))  + zeros(Npx, Npy)\n\nusing Plots\nheatmap(P_orig, title=\"Heatmap of original image\")\nheatmap(P_noisy, title=\"Heatmap of image with noise\")","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"Now we can modify the images.","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"P_mod = 0.5*P_noisy + P_noisy[end:-1:1,:]\nheatmap(P_mod, title=\"Modified noisy image\")","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"We can also implement and apply convolution.","category":"page"},{"location":"intro_programming/arrays_loops/","page":"Arrays and Loops","title":"Arrays and Loops","text":"function convolution_img(M)\n    Nx,Ny = size(M)\n\n    Mnew = similar(M)\n\n    for j in 2:Ny-1, i in 2:Nx-1\n        Mnew[i,j] = 5*M[i,j] - (M[i-1,j] + M[i+1,j] + M[i,j-1] + M[i,j+1])\n    end\n\n    for i in 2:Nx-1\n        Mnew[i,1] = 5*M[i,1] - (M[i-1,1] + M[i+1,1] + M[i,2] + Mnew[i,1])\n        Mnew[i,end] = 5*M[i,end] - (M[i-1,1] + M[i+1,1] + M[i,end-1] +  Mnew[i,end])\n    end\n\n    for j in 2:Ny-1\n        Mnew[1,j] = 5*M[1,j] - (M[2,j] + M[1,j-1] + M[1,j+1] + Mnew[1,j])\n        Mnew[end,j] = 5*M[end,j] - (M[end-1,j] + M[end,j-1] + M[end,j+1] + Mnew[end,j])\n    end\n\n    Mnew[1,1] = 5*M[1,1] - (M[2,1] + M[1,2] + 2*M[1,1])\n    Mnew[1,end] = 5*M[1,end] - (M[2,end] + M[1,end-1] + 2*M[1,end])\n    Mnew[end,1] = 5*M[end,1] - (M[end-1,1] + M[end,2]+ 2*M[end,1])\n    Mnew[end,end] = 5*M[end,end] - (M[end-1,end] + M[end,end-1] +  + 2*M[end,end])\n\n    return Mnew\nend\n\nP_conv = convolution_img(P_orig) # 1. Convolution\nP_conv = convolution_img(P_conv) # 2. Convolution\nheatmap(P_conv, title=\"Convolution of image\")","category":"page"},{"location":"basics_sciml/art_neural_networks/#Artificial-Neural-Networks","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"... are a set of machine learning tools which are \"inspired by biological neural networks\", see Wikipedia.  They can be visualized as (directed) graphs where the nodes or vertices are neurons and the edges are connections between the neurons.","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In this course we will focus on (simple) feed-forward networks with","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"one input layer consisting of N input neurons\na certain number of hidden layers, and\none ouput layer consisting of M output neurons.","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The j-th neuron of a hidden layer or output layer l has three components:","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"a connection from the k-th neuron of the previous layer l-1 with weight w_jk^l,\na bias b_j^l and\nan activation function f^l(cdot) like tanh or sigmoid function.","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The state of this neuron is computed as ","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"x_j^l = f^l(w_jk^l  x_k^l-1 + b_j^l)","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The weights and the bias are usually expressed in matrix and vector notation as ","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"W^l = beginpmatrix\nw_11  w_12  cdots  w_1k  cdots  w_1K \nvdots   vdots                          vdots  \nw_j1  w_j2  cdots  w_jk  cdots  w_jK \nvdots   vdots                           vdots  \nw_J1  w_J2  cdots  w_Jk  cdots  w_JK\nendpmatrix qquad b^l = beginpmatrix\nb_11 \nvdots  \nb_j1 \nvdots  \nb_J1 \nendpmatrix","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Weights and bias are parameters which are adjusted during the training of the neural network to minimize the difference between the recent output data y and the target output y_target. ","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Procedure","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Multiply the input data x^0 with the weights between the input layer and first hidden layer as W^1 x^0 + b^1\nCalculate output of the first hidden layer x^1 = f^1(W^1 x^0 +  + b^1)\nRepeat step 1 and 2 for all hidden layers and the output layers","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Therefore the output data after the last layer L is found as ","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = f^L( W^L f^L-1( W^L-1 f^L-2( cdots f^1(W^1 x^0 + b^1 ) + b^L-2 ) + b^L-1 ) + b^L )","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The computed output data y is compared with the target output y_target, and the difference is evaluated with a loss function or cost function. For example we use the quadratic loss function","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"J(y_targetiy) = sum_i=1 (y_targeti - y_i)^2","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The weights are adjusted and the loss function is minimized using backpropagation and optimization methods like (stochastic) gradient descent. ","category":"page"},{"location":"basics_sciml/art_neural_networks/#Linear-regression-in-1D","page":"Artificial Neural Networks","title":"Linear regression in 1D","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The first example is taken from the Flux documentation. Here, we try to approximate the function","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"f(x) = 4x + 2","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"with an artificial neural network with 1 input and 1 output neuron. The artificial neural network model can be noted as","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = W^1 x^0 + b^1","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"where W^1 and b^1 are the parameters that shall be learned and x^0 is the input data.","category":"page"},{"location":"basics_sciml/art_neural_networks/#Linear-regression-in-2D","page":"Artificial Neural Networks","title":"Linear regression in 2D","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the second example we try to approximate the two-dimensinal function","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"f(xy) = 3x - 5y + 2","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"with the artificial neural network","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = (W_11^1 W_12^1) beginpmatrix\nx_1^0\nx_2^0\nendpmatrix + b^1","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Here we have 2 input neurons and 1 output neuron. Next, we see how to build an ANN model step-by-step.","category":"page"},{"location":"basics_sciml/art_neural_networks/#Generate-data","page":"Artificial Neural Networks","title":"Generate data","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Firstly, we define the true model and create random input values for training and test. Here, we define the interval for training data as -34 and for test data as 29. The lower and upper bounds of the intervals are chosen arbitrarily. ","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"actual(x) = 3x[1] - 5x[2] + 2  # Model that shall be approximated\nN_samples = 10\nx_train, x_test = rand(-3:0.1:4, 2, N_samples), rand(2:0.1:9, 2, N_samples) # Input data","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Next, we need to generate the output data which is used later to compute the loss. ","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y_train, y_test = zeros(1,N_samples), zeros(1,N_samples) # Output data\nfor i in 1:N_samples\n    y_train[i] = actual(x_train[:,i])\n    y_test[i] = actual(x_test[:,i])\nend","category":"page"},{"location":"basics_sciml/art_neural_networks/#ANN-model-and-loss-function","page":"Artificial Neural Networks","title":"ANN model and loss function","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the second step, we create the artificial neural network model. As discussed above, we have 2 input neurons and 1 output neuron. We have in total 3 parameters: W_11^1 W_12^1 and b_1^1. We do not need an activation function because our reference model is linear.","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"using Flux\npredict = Dense(2 => 1) # ANN with 2 input neuron and 1 output neuron\nparameters = Flux.params(predict) # Parameters of ANN","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Further, we use a mean squared error as","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"J(y_ty) = sum (y_ti - y_i)^2","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"loss(x, y) = Flux.Losses.mse(predict(x), y); # Loss function: mean squared error","category":"page"},{"location":"basics_sciml/art_neural_networks/#Train-model","page":"Artificial Neural Networks","title":"Train model","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the last step, we choose gradient descent as the optimization method, define the input and output training data and run 1 training iteration.","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"using Flux: train!\nopt = Descent() # Gradient descent optimizer\ndata = [(x_train, y_train)]\ntrain!(loss, parameters, data, opt)","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The training step has to be repeated iteratively to minimize the loss.","category":"page"},{"location":"basics_sciml/art_neural_networks/#Nonlinear-function-approximation","page":"Artificial Neural Networks","title":"Nonlinear function approximation","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The approximation of linear functions is very simple because we only do not need a hidden layer - input and output layer are enough. In case of nonlinear functions we need hidden layers and activation functions. In this example we assume the nonlinear function","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"f(x) = 3 - 05x - 15  x^2 + 05  x^3","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In this example, we use  an artificial neural network with one hidden layer. The ANN structure is:","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Input layer with 1 neuron\n2 hidden layers with N neurons\nOutput layer with 1 neuron","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The ANN model is modelled with Chain() with N in the hidden layer:","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Nn = 64 # 16 # Number of neurons per layer\nmodel = Chain(\n  Dense(1 => Nn, σ),\n  Dense(Nn => Nn, σ),\n  Dense(Nn => 1))","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In the output layer L we sum up all results of the previous layer L-1 like","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"y = sum_n W_1n^L  f_n^L-1(W^L-1 x^L-2 + b^L-1) + b_11^L ","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Learning the weights and bias is typical function approximation or interpolation. And the more neurons we have in the hidden layer the more exact is our interpolation. However, this approach only works fine here for interpolation and not for extrapolation. ","category":"page"},{"location":"basics_sciml/art_neural_networks/#Flux-vs.-Lux","page":"Artificial Neural Networks","title":"Flux vs. Lux","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"Flux is one Julia's standard machine learning libraries. Beside Flux there exist several other libraries like:","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"MLJ\nKnet\nLux\nSimpleChains","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"In this course we will use Flux and Lux. Lux is a young project that was created to build a better foundation for Scientific Machine Learning methods than Flux. This means, the interoperability with DifferentialEquations framework shall be better than with Flux.","category":"page"},{"location":"basics_sciml/art_neural_networks/#Further-reading","page":"Artificial Neural Networks","title":"Further reading","text":"","category":"section"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"The Knet documentation contains a rather comprehensive introduction to artificial neural networks and their training methods. It can be found here: Backpropagation and SGD","category":"page"},{"location":"basics_sciml/art_neural_networks/","page":"Artificial Neural Networks","title":"Artificial Neural Networks","text":"If you want to play more with Flux, you may take a look on the Flux model zoo.","category":"page"},{"location":"intro_programming/finite_diff_2d/#Finite-differences-in-2-dimensions","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"","category":"section"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"We can also compute the gradient of a 2-dimensional function like","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"f(xy) = expleft(- (x^2 + y^2) right)","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"Firstly, we define the function","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"f(x,y) = exp(-(x^2 + y^2))","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"and generate the 2-dimensional data of this function.","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"Nx, Ny = 61, 61;        # Number of grid points\nxgrid  = range(-3,3,Nx) # Grid of x-values \nygrid  = range(-3,3,Ny) # Grid of y-values \nM      = zeros(Nx,Ny)   # Data values\n\nfor (j,ely) in enumerate(ygrid), (i,elx) in enumerate(xgrid)\n    M[i,j] = f(elx,ely)\nend","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"We calculate the gradient analytically as ","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"nabla f(xy) =\nbeginpmatrix\nfracddxf(xy) \nfracddyf(xy)\nendpmatrix =\n -2\nbeginpmatrix\nx \ny\nendpmatrix\nexpleft(- (x^2 + y^2) right)","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"So, we see that the gradient is a 2-dimensional vector. The finite differences are computed as in the 1-dimensional case - but here with a matrix instead of a vector.","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"# x-direction\nfunction finite_diff_2d_x(data2d,dx)    # 2-dimensional finite differences: x-direction\n    Lx,Ly  = size(data2d)               # Size of original matrix\n    diff2d = zeros(Lx-1,Ly);            # Matrix to store finite difference values\n    for j in 1:Ly, i in 1:Lx-1\n        diff2d[i,j] = (data2d[i+1,j]-data2d[i,j])/dx # Forward difference scheme\n    end\n    return diff2d\nend\n\n# y-direction\nfunction finite_diff_2d_y(data2d,dy)    # 2-dimensional finite differences: y-direction\n    Lx,Ly  = size(data2d)               # Size of original matrix\n    diff2d = zeros(Lx,Ly-1);            # Matrix to store finite difference values\n    for j in 1:Ly-1, i in 1:Lx\n        diff2d[i,j] = (data2d[i,j+1]-data2d[i,j])/dy # Forward difference scheme\n    end\n    return diff2d\nend","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"We only need to define the spatial sampling Delta x and Delta y and call the methods.","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"dx = xgrid[2] - xgrid[1]    # Spatial sampling: x-direction\ndy = ygrid[2] - ygrid[1]    # Spatial sampling: y-direction\nM_diffx = finite_diff_2d_x(M,dx) # Differentiation in x-direction\nM_diffy = finite_diff_2d_y(M,dy) # ... in y-direction\n","category":"page"},{"location":"intro_programming/finite_diff_2d/#Second-order-derivatives","page":"Finite differences in 2 dimensions","title":"Second-order derivatives","text":"","category":"section"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"The second-order derivative of a two-dimensional function is found analytically as","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"Delta f(xy) = fracd^2dx^2f(xy) + fracd^2dx^2f(xy)","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"and so we yield for our example function f(xy) = expleft(- (x^2 + y^2) right) the second-order derivative ","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"Delta f(xy) = (-4 + 4x^2 +4y^2)expleft(- (x^2 + y^2)right)","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"If we wish to compute second-order derivatives for any data we need to use numerical methods again. Second-order derivatives can be approximated with second-order central finite differences as","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"fracd^2dx^2f(x) approx fracf(x- Delta x) -2 f(x) + f(x+ Delta x)Delta x^2 = fracf(x_n-1) -2 f(x_n) + f(x_n+1)Delta x^2","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"We implement the second-order differentiation with two for-loops to iterate over the rows (inner loop) and the colums (outer loop). ","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"function diffusion2d(data2d,dx,dy)\n    Lx,Ly  = size(data2d)    # Size of original matrix\n    diff2d = zeros(Lx,Ly);   # Matrix to store finite difference values\n    for j in 2:Ly-1, i in 2:Lx-1\n        ddx = (data2d[i-1,j] + data2d[i+1,j] - 2*data2d[i,j])/dx^2  # 2nd order finite differences\n        ddy = (data2d[i,j-1] + data2d[i,j+1] - 2*data2d[i,j])/dy^2\n        diff2d[i,j] = ddx + ddy \n    end\n    return diff2d\nend\n\nM_diff2 = diffusion2d(M,dx,dy) # 2nd order differentiation","category":"page"},{"location":"intro_programming/finite_diff_2d/#Diffusion","page":"Finite differences in 2 dimensions","title":"Diffusion","text":"","category":"section"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"When we iteratively calculate the second-order derivatives and add them to the original matrix then we yield a diffusion. We may not this mathematically as","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"f^n+1(xy) = f^n(xy) + eta  fracd^2dx^2f^n(xy) + fracd^2dx^2f^n(xy)","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"where eta in (01) is a factor that has to be chosen. If eta is very small than our diffusion is slow - if it is too large then the numerical algorithm becomes unstable. We can find a suitable eta with the condition","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"1 - eta left(frac2Delta x^2 + frac2Delta y^2right)  0","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"which can be reformulated as","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"eta  left(frac2Delta x^2 + frac2Delta y^2right)^-1","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"In Julia we find this with","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"η_max = 1/(2*(1/dx^2  + 1/dy^2))","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"We set our scaling factor as η = 0.9*η_max and run the diffusion for 200 iterations.","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"M_blurr = copy(M)\nfor i=1:N_diff\n    M_blurr = M_blurr + η*diffusion2d(M_blurr,dx,dy)\nend","category":"page"},{"location":"intro_programming/finite_diff_2d/#Image-processing","page":"Finite differences in 2 dimensions","title":"Image processing","text":"","category":"section"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"Finally, we apply the diffusion on our apple image.","category":"page"},{"location":"intro_programming/finite_diff_2d/","page":"Finite differences in 2 dimensions","title":"Finite differences in 2 dimensions","text":"using Images\n# This image is taken from: https://openmoji.org/ \n# See: https://openmoji.org/library/emoji-1F34F/\nimg = load(\"images/1F34F_color.png\");\nNpx, Npy = size(img)\n\nP = channelview(Gray.(img)) + zeros(Npx, Npy)\nP_diff2d = diffusion2d(P,1,1)\nheatmap(P, title=\"Original image\")\nheatmap(P_diff2d, title=\"Diffusion of image\")\n\n# Blurring\niter_blur = 1000;\nP_new = copy(P)\nfor i=1:iter_blur\n    P_new = P_new + 0.1 * diffusion2d(P_new,1,1)\nend\n\nblur_title = string(\"Blurring after n=\", iter_blur, \" iterations\")\nheatmap(P_new, title=blur_title)","category":"page"},{"location":"basics_sciml/homework_2/#Homework:-Machine-Learning","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"","category":"section"},{"location":"basics_sciml/homework_2/#dimensional-function-approximation","page":"Homework: Machine Learning","title":"2-dimensional function approximation","text":"","category":"section"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Read in the Flux documentation about building models:","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Overview\nBasics ","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"We assume the true model","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"    f(xy) = 200xfraccos(2x)exp(-x^2)1 + exp(-2x)sin(yexp(-y^2)) ","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Tasks","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Create random training values for (xy) in -22 times -22\nBuild an artificial neural network model (with Flux) with at least 1 hidden layer\nTrain the ANN model and plot the loss values per iteration. Use at least 5000 iterations.\nPlot the graph of the ANN model and the true model.","category":"page"},{"location":"basics_sciml/homework_2/#Function-approximation-with-Lux.jl","page":"Homework: Machine Learning","title":"Function approximation with Lux.jl","text":"","category":"section"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Read in the Lux documentation:","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Julia & Lux for the Uninitiated\nFitting a Polynomial using MLP","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Task","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"Implement with Lux.jl the 1-dimensional function approximation example with","category":"page"},{"location":"basics_sciml/homework_2/","page":"Homework: Machine Learning","title":"Homework: Machine Learning","text":"y = sum_n W_1n^L  f_n^L-1(W^L-1 x^L-2 + b^L-1) + b_11^L","category":"page"},{"location":"projects/#Course-Projects","page":"Projects","title":"Course Projects","text":"","category":"section"},{"location":"projects/#Augmented-Neural-Ordinary-Differential-Equations","page":"Projects","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint","category":"page"},{"location":"projects/#Continuous-Normalizing-Flows-/-FFJORD","page":"Projects","title":"Continuous Normalizing Flows / FFJORD","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia:Flow-based generative model","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint: Neural ODE -> About Continuous Normalizing Flows","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint: FFJORD","category":"page"},{"location":"projects/#Hamiltonian-Neural-Network","page":"Projects","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Arxiv Preprint","category":"page"},{"location":"projects/#Physics-Informed-Machine-Learning-(PIML)-with-TensorLayer","page":"Projects","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/#Neural-Graph-Differential-Equations","page":"Projects","title":"Neural Graph Differential Equations","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"GraphNeuralNetworks.jl","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Graph neural network","category":"page"},{"location":"projects/#Partial-Differential-Equation-(PDE)-Constrained-Optimization","page":"Projects","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Heat equation","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Finite Differences method","category":"page"},{"location":"projects/#Solving-Optimal-Control-Problems-with-Universal-Differential-Equations","page":"Projects","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"","category":"section"},{"location":"projects/","page":"Projects","title":"Projects","text":"Julia Tutorial","category":"page"},{"location":"projects/","page":"Projects","title":"Projects","text":"Wikipedia: Optimal Control","category":"page"},{"location":"basics_sciml/homework_1/#Homework:-Differential-Equations","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"","category":"section"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"Read the DifferentialEquations.jl documentation about Ordinary Differential Equations and solve the following exercises.","category":"page"},{"location":"basics_sciml/homework_1/#Linear-ODE","page":"Homework: Differential Equations","title":"Linear ODE","text":"","category":"section"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"We assume the ordinary differential equation","category":"page"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"dotx_1(t) =  -x_1 + 2  x_2 + 3  x_3 \ndotx_2(t) = 4x_1 - 5x_2 + 6x_3 \ndotx_3(t) = 7x_1 + 8x_2 - 9x_3","category":"page"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"Tasks","category":"page"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"Is this ODE stable? Compute the Eigenvalues.\nFind the analytical solution for initial values x(0)=(111)^top.\nDiscretize the system with Euler method and simulate it for T_f=5 seconds.\nPlot the analytical solution and the solution of the Euler method. You may use the option yaxis=:log10 in plot() for a better readability.\nCompute the solution with DifferentialEquations.jl or OrdinaryDiffEq.jl (which is a sub-library of DifferentialEquations.jl).","category":"page"},{"location":"basics_sciml/homework_1/#Mathematical-Pendulum","page":"Homework: Differential Equations","title":"Mathematical Pendulum","text":"","category":"section"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"We assume the mathematical pendulum","category":"page"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"dotphi(t) = omega(t) \ndotomega(t) = -fracgl  sin(phi(t))","category":"page"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"with angle phi, angle velocity omega, gravitational acceleration g=981 and lenght l0. ","category":"page"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"Tasks","category":"page"},{"location":"basics_sciml/homework_1/","page":"Homework: Differential Equations","title":"Homework: Differential Equations","text":"Compute the solution with DifferentialEquations.jl or OrdinaryDiffEq.jl\nAssume the initial values (phi_0omega_0)=(π40)^top and simulation time T_f=5 seconds.\nUse parameter p=fracgl with l=01.\nPlot your results.\nVary the length from l=01 to l=1 meter.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = SciMLMiniCourse","category":"page"},{"location":"#Mini-Course-on-Scientific-Machine-Learning-(SciML)","page":"Home","title":"Mini Course on Scientific Machine Learning (SciML)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This mini course is based on Julia programming language and covers the following topics","category":"page"},{"location":"","page":"Home","title":"Home","text":"Quick introduction / repetition of differential equations\nSee: DifferentialEquations.jl\nArtificial Neural Networks \nFlux.jl for standard Machine Learning \nLux.jl as ML backend for SciML\nNeural Ordinary Differential Equations \nDiffEqFlux.jl for Neural ODEs\nSciMLSensitivity.jl for Parameter Estimation and Sensitivity Analysis\nOptimization.jl Meta-Package for Optimization tools\nSensitivity Analysis\nForwardDiff.jl for Forward Differentiation\nZygote.jl for Automatic Differentiation","category":"page"},{"location":"#What-is-SciML?","page":"Home","title":"What is SciML?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In Scientific Machine Learning we use ideas and tools from machine learning like artificial neural network, optimizers or automatic differentiation and apply them on problems from natural sciences and engineering. In other words as noted on webpage scientific-ml.com: \"...the development and mathematical theory of machine learning techniques for applications in computational science and engineering\".","category":"page"},{"location":"","page":"Home","title":"Home","text":"In natural sciences and engineering we describe our ideas and models with formulas, for instance differential equations. We run experiments to gain data which we compare with our theoretical model. The more data we have the more precisly we can specify our model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Falling body example","category":"page"},{"location":"","page":"Home","title":"Home","text":"We assume we let an object fall down from x=10 meter, we measure the time until it hits the ground and we want to find the local gravity constant g. This model is described by formula  x(t) = fracg2  t^2 or equally g = frac2t^2  x.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Our time measurements and found gravity constants are","category":"page"},{"location":"","page":"Home","title":"Home","text":"Experiment Time [s] Gravity constant [fracms^2]\n1 1.51 8.77\n2 1.40 10.20\n3 1.39 10.35\n4 1.47 9.26\n5 1.44 9.65","category":"page"},{"location":"","page":"Home","title":"Home","text":"The mean gravity constant is overlineg = fracsum g_i5 approx 965  fracms^2. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"In case of such small and easy models the proposed manual computation may work well. However, in case of complex models like bio-chemical reactions, energy distribution networks or quantum mechanics with hundreds or thousands of formulas and parameters the computation is unknown variables or parameters is much more expensive. So, we need powerful computational tools to handle such problems. ","category":"page"},{"location":"#Learn-about-Julia","page":"Home","title":"Learn about Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are new to Julia then I recommend to learn about it:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Wikibooks: Introducing Julia\nBlog: The Julia Programming Language: an Effective Tutorial\nYoutube: Learn Julia with Us\nMany more resources can be found here","category":"page"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Computer with Windows/Linux/MacOS/FreeBSD\n(Latest) Julia installation, see: Download Julia\nText editor like Notepad++, Gedit, Visual Studio Code, (Atom)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Not required but can be also used:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Jupyter Notebooks, see IJulia.jl","category":"page"},{"location":"#Further-ressources-about-SciML","page":"Home","title":"Further ressources about SciML","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications\nIntroduction to Computational Thinking\nList of tutorials: SciMLTutorials.jl: Tutorials for Scientific Machine Learning and Differential Equations","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation for SciMLMiniCourse.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [SciMLMiniCourse]","category":"page"},{"location":"intro_programming/intro_julia/#Quick-introduction","page":"Quick introduction","title":"Quick introduction","text":"","category":"section"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"When you installed the Julia environment on your PC, please open the Julia executive. The REPL will show you something like","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"julia>","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"Here you can try typical commands like","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"julia> 1+1\n2\n\njulia> println(\"Hello World\")\nHello World\n\njulia> foo(x) = x\njulia> foo(3)\n3","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"If you want to develop scripts or packages then it you may use a text editor. You can write some code like","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"a = 3;\nb = 5;\nc = a + b\n\nfunction myfun(x,y)\n    return x+y\nend","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"Save your code, here \"mydemofile.jl\". Now you can open and run your script in the REPL.","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"julia> include(\"path/to_my_file/my_demo_file.jl\")","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"Now, the variables a, b and c and the function myfun are in your environment.","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"julia> a\n3\n\njulia> b\n5\n\njulia> c\n8\n\njulia> myfun(4,5)\n9","category":"page"},{"location":"intro_programming/intro_julia/#Visual-Studio-Code","page":"Quick introduction","title":"Visual Studio Code","text":"","category":"section"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"High level editors like VS Code simplify the usage of Julia. VS Code supports Julia with its extension. See also its documentation for the installation guide. ","category":"page"},{"location":"intro_programming/intro_julia/#Installing-Packages","page":"Quick introduction","title":"Installing Packages","text":"","category":"section"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"Julia has an own Package Manager called Pkg. You can reach it in the REPL if you press ] and you can leave it with backspace. If you want to install a package, e.g. the standard plotting library  Plots then open the REPL and...","category":"page"},{"location":"intro_programming/intro_julia/","page":"Quick introduction","title":"Quick introduction","text":"julia> ]\n\npkg>\n\npkg> add Plots","category":"page"}]
}
